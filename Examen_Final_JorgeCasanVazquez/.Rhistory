if (any((1 + xi * (data - u) / tau) <= 0))
return(1e6)
m * log(tau) + (1 + 1 / xi) *
sum(log(1 + xi * (data - u) / tau))
}
}
# Obtención de los parámetros PARETO
tau.start <- mean(exceden_umbral) - u #valores medios entre el umbral y los excesos
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
data = exceden_umbral)
fit.gp
fit.gp$estimate
sqrt(diag(solve(fit.gp$hessian)))
prof.nllik.gp <- function(par,xi, u, data)
nllik.gp(c(par,xi), u, data)
prof.fit.gp <- function(x)
-nlm(prof.nllik.gp, tau.start, xi = x, u = u, hessian = TRUE,
data = exceden_umbral)$minimum #ojo cambiar base de datos
vxi = seq(0,1.8,by = .025)
prof.lik <- Vectorize(prof.fit.gp)(vxi)
plot(vxi, prof.lik, type = "l", xlab = expression(xi),
ylab = "Profile log-likelihood")
opt <- optimize(f = prof.fit.gp, interval = c(0,3), maximum = TRUE)
opt
up <- opt$objective
abline(h = up, lty = 2)
abline(h = up - qchisq(p = 0.95, df = 1), col = "grey")
I <- which(prof.lik >= up - qchisq(p = 0.95, df = 1))
lines(vxi[I], rep(up - qchisq(p = 0.95, df = 1), length(I)),
lwd = 5, col = "grey")
abline(v = range(vxi[I]), col = "grey", lty = 2)
abline(v = opt$maximum, col = "grey")
# A) Q-Q Plot para la Dist. Generalizada de Pareto (DGP)
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
qqgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
ppgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
ppgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Graficar la función de pérdidas acumuladas
png("./imagenes/funcion_perdidas_agregadas.png", width = 800, height = 500)
plot(perdidas_agregadas)
prof.nllik.gp <- function(par,xi, u, data)
nllik.gp(c(par,xi), u, data)
prof.fit.gp <- function(x)
-nlm(prof.nllik.gp, tau.start, xi = x, u = u, hessian = TRUE,
data = exceden_umbral)$minimum #ojo cambiar base de datos
vxi = seq(0,1.8,by = .025)
prof.lik <- Vectorize(prof.fit.gp)(vxi)
plot(vxi, prof.lik, type = "l", xlab = expression(xi),
ylab = "Profile log-likelihood")
opt <- optimize(f = prof.fit.gp, interval = c(0,3), maximum = TRUE)
opt
up <- opt$objective
abline(h = up, lty = 2)
abline(h = up - qchisq(p = 0.95, df = 1), col = "grey")
I <- which(prof.lik >= up - qchisq(p = 0.95, df = 1))
lines(vxi[I], rep(up - qchisq(p = 0.95, df = 1), length(I)),
lwd = 5, col = "grey")
abline(v = range(vxi[I]), col = "grey", lty = 2)
abline(v = opt$maximum, col = "grey")
# A) Q-Q Plot para la Dist. Generalizada de Pareto (DGP)
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
qqgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
ppgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Graficar la función de pérdidas acumuladas
png("./imagenes/funcion_perdidas_agregadas.png", width = 800, height = 500)
plot(perdidas_agregadas)
ano1<- datos[1:12,]
ano2<- datos[13:24,]
ano3<- datos[25:36,]
ano4<- datos[37:42,]
ano5<- datos[43:60,]
maximos<- c(max(ano1),max(ano2),max(ano3),max(ano4),max(ano5))
df_maximos<- data.frame(Year1= maximos[1], Year2= maximos[2], Year3=maximos[3], Year4=maximos[4],Year5=maximos[5])
df_maximos
u <- mean(datos$severidad)
u
exceden_umbral <- datos$severidad[datos$severidad > u]
exceden_umbral
#Hay N casos que superan el umbral
l = length(exceden_umbral)
surv.prob <- 1 - base::rank(exceden_umbral)/(l + 1)  #rank ofrece el n de orden
surv.prob
plot(exceden_umbral, surv.prob, log = "xy", xlab = "Excesos",
ylab = "Probabilidades", ylim = c(0.01, 1))
#Determinamos los parámetros necesarios
alpha <- -cov(log(exceden_umbral), log(surv.prob)) / var(log(exceden_umbral))
alpha
x = seq(u, max(exceden_umbral), length = 100) #divide de u a max() 100 interv.
y = (x / u)^(-alpha)
lines(x, y)
#Función de distribución acumulada
prob <- rank(exceden_umbral) / (l + 1)
plot(exceden_umbral, prob, log = "x", xlab = "Excesos", ylab = "Probabilidades de no excesos")
y = 1 - (x / u)^(-alpha)
lines(x, y)
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
#Lectura de ficheros
frecuencias<- read.csv('frecuencias_0619.csv')
cuantias<- read.csv2('cuantias_0619.csv', header = T)
#Unimos los dos datasets
datos<- cbind(frecuencias, cuantias)
names(datos)<- c('freq', 'severidad')
#Frecuencias
table(datos$freq)
summary(datos$freq) #Alta concentración de valores en el punto medio, la media y mediana no son iguales, valores atípicos
var(datos$freq) #Existe una alta varianza en la frecuencia mensual
frecuencias<- read.csv('frecuencias_0619.csv')
View(frecuencias)
View(frecuencias)
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
#Lectura de ficheros
frecuencias<- read.csv('frecuencias_0619.csv')
cuantias<- read.csv2('cuantias_0619.csv', header = T)
#Unimos los dos datasets
datos<- cbind(frecuencias, cuantias)
names(datos)<- c('freq', 'severidad')
#Frecuencias
table(datos$freq)
summary(datos$freq) #Alta concentración de valores en el punto medio, la media y mediana no son iguales, valores atípicos
var(datos$freq) #Existe una alta varianza en la frecuencia mensual
skewness(datos$freq) #Existe una asimetría en la distribución de los datos hacia la derecha
kurtosis(datos$freq) # Existe un apuntamiento en la distribución de los datos, leptocúrtica. No sigue una distribución normal
boxplot(datos$freq) # Alta concetración de valores en el punto medio de la distribución, con presencia de valores atípicos
#Realizamos el análisis de los cuartiles
quantile(datos$freq, probs = c(0.05, 0.95))
quantile(datos$freq, seq(0,1, 0.20))# A partir del decil 80 se evidencia la presencia de valores extremos
quantile(datos$freq, seq(0.9,1, 0.01)) # A partir del percentil 96 se evidencia la presencia de valores extremos significativos
#Graficamos la distribución de la frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
ggplot(datos) + geom_density(aes(datos$freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
#Severidad
#Para hacer el análisis de la severidad tendremos que trabajar con las cuantías
table(datos$severidad)
summary(datos$severidad) #Alta concentración de valores en el punto medio, la media y mediana son claramente diferentes
var(datos$severidad) #Existe una alta varianza en la frecuencia mensual, alta presencia de valores atípicos
skewness(datos$severidad) #Existe una asimetría en la distribución de los datos hacia la derecha
kurtosis(datos$severidad) # Existe un apuntamiento en la distribución de los datos, leptocúrtica. No sigue una distribución normal
#Realizamos el análisis de los cuartiles
quantile(datos$severidad, probs = c(0.05, 0.95))
quantile(datos$severidad, seq(0,1, 0.20))# A partir del decil 60 se evidencia la presencia de valores extremos
quantile(datos$severidad, seq(0.9,1, 0.01)) # A partir del percentil 98 se evidencia la presencia de valores extremos significativos
boxplot(datos$severidad)
png("./imagenes/distribucion severidad.png", width = 800, height = 500)
ggplot(datos) + geom_density(aes(severidad), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
fpois = fitdist(datos$freq, "pois", method = 'mle')
summary(fpois)
plot(fpois)
# Distribucion  Binomial Negativa, metodo máxima verosimilitud
fnbinom = fitdist(datos$freq, "nbinom", 'mle')
summary(fnbinom)
plot(fnbinom)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
cdfcomp(list(fpois, fnbinom))
# Pruebas de bondad de ajuste
ba_discreta <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
ba_discreta
# Pruebas de bondad de ajuste
ba_discreta <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
ba_discreta
# Estadítico de bondad de ajuste: Chi test(pvalue)
ba_discreta$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
# Criterio de bondad de ajuste (AIC y BIC)
ba_discreta$aic; ba_discreta$bic #se escoge la Bino. Neg por menor aic y bic
# Resultado: La distribucion de frecuencia se ajusta mejor
# con una binomial negativa
size = fnbinom$estimate[1]
size
mu = fnbinom$estimate[2]
mu
# GAMMA
fgam <- fitdist(datos$severidad, "gamma", lower = 0)
summary(fgam)
plot(fgam)
# PARETO
fpar <- fitdist(datos$severidad, "pareto", start = list(shape = 2, scale = 3))
summary(fpar)
plot(fpar)
# BURR
fburr <- fitdist(datos$severidad, "burr", method = 'mle', start = list(shape1 = 1, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Pruebas de bondad de ajuste
ba_continua <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr"))
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
#Lectura de ficheros
frecuencias<- read.csv('frecuencias_0619.csv')
cuantias<- read.csv2('cuantias_0619.csv', header = T)
#Unimos los dos datasets
datos<- cbind(frecuencias, cuantias)
names(datos)<- c('freq', 'severidad')
#Frecuencias
table(datos$freq)
summary(datos$freq) #Alta concentración de valores en el punto medio, la media y mediana no son iguales, valores atípicos
var(datos$freq) #Existe una alta varianza en la frecuencia mensual
skewness(datos$freq) #Existe una asimetría en la distribución de los datos hacia la derecha
kurtosis(datos$freq) # Existe un apuntamiento en la distribución de los datos, leptocúrtica. No sigue una distribución normal
boxplot(datos$freq) # Alta concetración de valores en el punto medio de la distribución, con presencia de valores atípicos
#Realizamos el análisis de los cuartiles
quantile(datos$freq, probs = c(0.05, 0.95))
quantile(datos$freq, seq(0,1, 0.20))# A partir del decil 80 se evidencia la presencia de valores extremos
quantile(datos$freq, seq(0.9,1, 0.01)) # A partir del percentil 96 se evidencia la presencia de valores extremos significativos
#Graficamos la distribución de la frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
ggplot(datos) + geom_density(aes(datos$freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
#Severidad
#Para hacer el análisis de la severidad tendremos que trabajar con las cuantías
table(datos$severidad)
summary(datos$severidad) #Alta concentración de valores en el punto medio, la media y mediana son claramente diferentes
var(datos$severidad) #Existe una alta varianza en la frecuencia mensual, alta presencia de valores atípicos
skewness(datos$severidad) #Existe una asimetría en la distribución de los datos hacia la derecha
kurtosis(datos$severidad) # Existe un apuntamiento en la distribución de los datos, leptocúrtica. No sigue una distribución normal
#Realizamos el análisis de los cuartiles
quantile(datos$severidad, probs = c(0.05, 0.95))
quantile(datos$severidad, seq(0,1, 0.20))# A partir del decil 60 se evidencia la presencia de valores extremos
quantile(datos$severidad, seq(0.9,1, 0.01)) # A partir del percentil 98 se evidencia la presencia de valores extremos significativos
boxplot(datos$severidad)
png("./imagenes/distribucion severidad.png", width = 800, height = 500)
ggplot(datos) + geom_density(aes(severidad), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
fpois = fitdist(datos$freq, "pois", method = 'mle')
summary(fpois)
plot(fpois)
# Distribucion  Binomial Negativa, metodo máxima verosimilitud
fnbinom = fitdist(datos$freq, "nbinom", 'mle')
summary(fnbinom)
plot(fnbinom)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
cdfcomp(list(fpois, fnbinom))
# Pruebas de bondad de ajuste
bondad_frecuencias <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
bondad_frecuencias
# Estadítico de bondad de ajuste: Chi test(pvalue)
bondad_frecuencias$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
# Criterio de bondad de ajuste (AIC y BIC)
bondad_frecuencias$aic; bondad_frecuencias$bic #se escoge la Bino. Neg por menor aic y bic
# Resultado: La distribucion de frecuencia se ajusta mejor
# con una binomial negativa
size = fnbinom$estimate[1]
size
mu = fnbinom$estimate[2]
mu
# GAMMA
fgam <- fitdist(datos$severidad, "gamma", lower = 0)
summary(fgam)
plot(fgam)
# PARETO
fpar <- fitdist(datos$severidad, "pareto", start = list(shape = 2, scale = 3))
summary(fpar)
plot(fpar)
# BURR
fburr <- fitdist(datos$severidad, "burr", method = 'mle', start = list(shape1 = 1, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Pruebas de bondad de ajuste
bondad_severidad <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr"))
# Estadítico de bondad de ajuste: Kolmogorov-Smirnov statistic
bondad_severidad$ks
bondad_severidad$kstest
#Criterio de bondad de ajuste
bondad_severidad$aic; bondad_severidad$bic
# Comprobacion grafica entre las distribuciones continuas
FDD = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("black", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
# La distribución se ajusta mejor a burr
png("./imagenes/ajuste_severidad.png", width = 800, height = 500)
FDD
# Resultado: La distribucion de severidad se ajusta mejor
# con una distribución burr
shape1 = fburr$estimate[1]
shape2 = fburr$estimate[2]
scale1 = fburr$estimate[3]
# Resultado: La distribucion de severidad se ajusta mejor
# con una distribución burr
shape1 = fburr$estimate[1]
shape2 = fburr$estimate[2]
scale1 = fburr$estimate[3]
u <- mean(datos$severidad)
u
exceden_umbral <- datos$severidad[datos$severidad > u]
exceden_umbral
#Hay N casos que superan el umbral
l = length(exceden_umbral)
surv.prob <- 1 - base::rank(exceden_umbral)/(l + 1)  #rank ofrece el n de orden
surv.prob
plot(exceden_umbral, surv.prob, log = "xy", xlab = "Excesos",
ylab = "Probabilidades", ylim = c(0.01, 1))
#Determinamos los parámetros necesarios
alpha <- -cov(log(exceden_umbral), log(surv.prob)) / var(log(exceden_umbral))
alpha
x = seq(u, max(exceden_umbral), length = 100) #divide de u a max() 100 interv.
y = (x / u)^(-alpha)
lines(x, y)
#Función de distribución acumulada
prob <- rank(exceden_umbral) / (l + 1)
plot(exceden_umbral, prob, log = "x", xlab = "Excesos", ylab = "Probabilidades de no excesos")
y = 1 - (x / u)^(-alpha)
lines(x, y)
#Distribucion valores extremos generalizados (GEV)
nllik.gev <- function(par, data){
mu <- par[1]
sigma <- par[2]
xi <- par[3]
if ((sigma <= 0) | (xi <= -1))
return(1e6)
n <- length(data)
if (xi == 0)
n * log(sigma) + sum((data - mu) / sigma) +
sum(exp(-(data - mu) / sigma))
else {
if (any((1 + xi * (data - mu) / sigma) <= 0))
return(1e6)
n * log(sigma) + (1 + 1 / xi) *
sum(log(1 + xi * (data - mu) / sigma)) +
sum((1 + xi * (data - mu) / sigma)^(-1/xi))
}
}
# GEV
sigma.start <- sqrt(6) * sd(exceden_umbral) / pi
mu.start <- mean(exceden_umbral) + digamma(1) * sigma.start
fit.gev <- nlm(nllik.gev, c(mu.start, sigma.start, 0),
hessian = TRUE, data = exceden_umbral)
fit.gev
fit.gev$estimate
sqrt(diag(solve(fit.gev$hessian)))
# El indice de cola es significativo y positivo en GEV
# B) Modelo Poisson-Generalizada de Pareto
nllik.gp <- function(par, u, data){
tau <- par[1]
xi <- par[2]
if ((tau <= 0) | (xi < -1))
return(1e6)
m <- length(data)
if (xi == 0)
m * log(tau) + sum(data - u) / tau
else {
if (any((1 + xi * (data - u) / tau) <= 0))
return(1e6)
m * log(tau) + (1 + 1 / xi) *
sum(log(1 + xi * (data - u) / tau))
}
}
# Obtención de los parámetros PARETO
tau.start <- mean(exceden_umbral) - u #valores medios entre el umbral y los excesos
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
data = exceden_umbral)
fit.gp
fit.gp$estimate
sqrt(diag(solve(fit.gp$hessian)))
prof.nllik.gp <- function(par,xi, u, data)
nllik.gp(c(par,xi), u, data)
prof.fit.gp <- function(x)
-nlm(prof.nllik.gp, tau.start, xi = x, u = u, hessian = TRUE,
data = exceden_umbral)$minimum #ojo cambiar base de datos
vxi = seq(0,1.8,by = .025)
prof.lik <- Vectorize(prof.fit.gp)(vxi)
plot(vxi, prof.lik, type = "l", xlab = expression(xi),
ylab = "Profile log-likelihood")
opt <- optimize(f = prof.fit.gp, interval = c(0,3), maximum = TRUE)
opt
up <- opt$objective
abline(h = up, lty = 2)
abline(h = up - qchisq(p = 0.95, df = 1), col = "grey")
I <- which(prof.lik >= up - qchisq(p = 0.95, df = 1))
lines(vxi[I], rep(up - qchisq(p = 0.95, df = 1), length(I)),
lwd = 5, col = "grey")
abline(v = range(vxi[I]), col = "grey", lty = 2)
abline(v = opt$maximum, col = "grey")
# A) Q-Q Plot para la Dist. Generalizada de Pareto (DGP)
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
qqgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
ppgpd(datos$severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Graficar la función de pérdidas acumuladas
png("./imagenes/funcion_perdidas_agregadas.png", width = 800, height = 500)
plot(perdidas_agregadas)
# Estadísticos de la nueva función:
summary(perdidas_agregadas)
# Deteminacion del Value at Risk al 0.9%
quantile(perdidas_agregadas, 0.90)
# Comprobacion grafica entre las distribuciones continuas
FDD = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("black", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
# La distribución se ajusta mejor a burr
png("./imagenes/ajuste_severidad.png", width = 800, height = 500)
FDD
# Deteminacion del Value at Risk al 90%
quantile(perdidas_agregadas, 0.90)
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
